#!/usr/bin/env python3
"""
BPE ByteLevel Tokenizer Vocab è§£ç å·¥å…·

ç”¨äºå°† ByteLevel BPE tokenizer çš„ vocab.json ä¸­çš„"ä¹±ç "token è¿˜åŸä¸ºåŸå§‹æ–‡æœ¬ã€‚
é€‚ç”¨äº Qwenã€GPT-2 ç­‰ä½¿ç”¨ ByteLevel ç¼–ç çš„æ¨¡å‹ã€‚
"""

import json
import argparse
from pathlib import Path


def bytes_to_unicode():
    """
    ç”Ÿæˆå­—èŠ‚åˆ°Unicodeå­—ç¬¦çš„æ­£å‘æ˜ å°„è¡¨
    è¿™ä¸ªæ˜ å°„æ¥è‡ª GPT-2 çš„ tokenizer å®ç°
    
    è¿”å›å­—å…¸ï¼š{byte_value: unicode_char}
    """
    # ä¿ç•™çš„å¯æ‰“å°å­—ç¬¦èŒƒå›´
    bs = (
        list(range(ord("!"), ord("~") + 1)) +      # ASCIIå¯æ‰“å°å­—ç¬¦ï¼ˆ33-126ï¼‰
        list(range(ord("Â¡"), ord("Â¬") + 1)) +      # æ‰©å±•å­—ç¬¦ï¼ˆ161-172ï¼‰
        list(range(ord("Â®"), ord("Ã¿") + 1))        # æ‰©å±•å­—ç¬¦ï¼ˆ174-255ï¼‰
    )
    
    cs = bs.copy()
    n = 0
    
    # å°†æœªè¢«ä¿ç•™çš„å­—èŠ‚æ˜ å°„åˆ°æ›´é«˜çš„Unicodeç ä½
    for b in range(2**8):  # éå†æ‰€æœ‰å­—èŠ‚ï¼ˆ0-255ï¼‰
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)  # æ˜ å°„åˆ°256+nçš„Unicodeç ä½
            n += 1
    
    # è½¬æ¢ä¸ºUnicodeå­—ç¬¦
    cs = [chr(code) for code in cs]
    
    return dict(zip(bs, cs))


def get_reverse_mapping(forward_map):
    """
    æ ¹æ®æ­£å‘æ˜ å°„ç”Ÿæˆåå‘æ˜ å°„
    
    è¿”å›å­—å…¸ï¼š{unicode_char: byte_value}
    """
    return {v: k for k, v in forward_map.items()}


def unicode_str_to_bytes(unicode_str, reverse_map):
    """
    å°†æ˜ å°„åçš„Unicodeå­—ç¬¦ä¸²è½¬æ¢å›åŸå§‹å­—èŠ‚åºåˆ—
    
    Args:
        unicode_str: æ˜ å°„åçš„Unicodeå­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼š"Ã¤Â½Å‚Ã¥Â¥Â½"ï¼‰
        reverse_map: åå‘æ˜ å°„è¡¨
    
    Returns:
        bytes: åŸå§‹å­—èŠ‚åºåˆ—
    """
    return bytes([reverse_map[c] for c in unicode_str])


def decode_token(token_str, reverse_map):
    """
    è§£ç å•ä¸ªtokenå­—ç¬¦ä¸²ä¸ºåŸå§‹æ–‡æœ¬
    
    Args:
        token_str: tokenå­—ç¬¦ä¸²ï¼ˆå¯èƒ½æ˜¯ä¹±ç ï¼‰
        reverse_map: åå‘æ˜ å°„è¡¨
    
    Returns:
        str: è§£ç åçš„åŸå§‹æ–‡æœ¬ï¼ˆå¦‚æœæ— æ³•è§£ç æˆ–è§£ç ä¸ºç©ºåˆ™è¿”å›åŸå­—ç¬¦ä¸²ï¼‰
    """
    try:
        # å°†Unicodeå­—ç¬¦ä¸²è½¬å›å­—èŠ‚
        byte_sequence = unicode_str_to_bytes(token_str, reverse_map)
        # å°è¯•è§£ç ä¸ºUTF-8æ–‡æœ¬
        decoded = byte_sequence.decode('utf-8', errors='ignore')
        # å¦‚æœè§£ç ç»“æœä¸ºç©ºï¼Œè¿”å›åŸå­—ç¬¦ä¸²
        if not decoded:
            return token_str
        return decoded
    except Exception:
        # å¦‚æœè§£ç å¤±è´¥ï¼Œè¿”å›åŸå­—ç¬¦ä¸²
        return token_str


def load_vocab(vocab_path):
    """
    åŠ è½½vocab.jsonæ–‡ä»¶
    
    Args:
        vocab_path: vocab.jsonçš„è·¯å¾„
    
    Returns:
        dict: tokenåˆ°IDçš„æ˜ å°„
    """
    with open(vocab_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def decode_vocab(vocab_path, output_path=None, show_samples=10):
    """
    è§£ç æ•´ä¸ªvocabæ–‡ä»¶
    
    Args:
        vocab_path: vocab.jsonçš„è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        show_samples: æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡
    """
    print("ğŸ”„ åŠ è½½æ˜ å°„è¡¨...")
    forward_map = bytes_to_unicode()
    reverse_map = get_reverse_mapping(forward_map)
    
    print(f"ğŸ“– åŠ è½½è¯æ±‡è¡¨: {vocab_path}")
    vocab = load_vocab(vocab_path)
    
    print(f"âœ¨ è§£ç  {len(vocab)} ä¸ªtokens...")
    decoded_vocab = {}
    
    for token_str, token_id in vocab.items():
        decoded = decode_token(token_str, reverse_map)
        decoded_vocab[token_str] = decoded
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nğŸ“Š æ˜¾ç¤ºå‰ {show_samples} ä¸ªè§£ç ç¤ºä¾‹ï¼š")
    print("-" * 80)
    print(f"{'ID':<8} {'åŸå§‹Token':<30} {'è§£ç å':<30}")
    print("-" * 80)
    
    # æŒ‰IDæ’åºæ˜¾ç¤º
    sorted_items = sorted(vocab.items(), key=lambda x: x[1])
    for i, (token_str, token_id) in enumerate(sorted_items):
        if i >= show_samples:
            break
        decoded = decoded_vocab[token_str]
        original = token_str[:28]  # æˆªæ–­è¿‡é•¿çš„å­—ç¬¦ä¸²
        decoded_display = decoded[:28]
        print(f"{token_id:<8} {original:<30} {decoded_display:<30}")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_path:
        print(f"\nğŸ’¾ ä¿å­˜è§£ç ç»“æœåˆ°: {output_path}")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(decoded_vocab, f, ensure_ascii=False, indent=2)
        print("âœ… ä¿å­˜å®Œæˆï¼")
    
    return decoded_vocab


def query_token(vocab_path, query):
    """
    æŸ¥è¯¢ç‰¹å®štokençš„è§£ç ç»“æœ
    
    Args:
        vocab_path: vocab.jsonçš„è·¯å¾„
        query: è¦æŸ¥è¯¢çš„tokenå­—ç¬¦ä¸²æˆ–ID
    """
    forward_map = bytes_to_unicode()
    reverse_map = get_reverse_mapping(forward_map)
    vocab = load_vocab(vocab_path)
    
    # å¦‚æœqueryæ˜¯æ•°å­—ï¼ŒæŒ‰IDæŸ¥æ‰¾
    if query.isdigit():
        token_id = int(query)
        for token_str, tid in vocab.items():
            if tid == token_id:
                decoded = decode_token(token_str, reverse_map)
                print(f"\nğŸ“Œ Token ID: {token_id}")
                print(f"   åŸå§‹: {token_str}")
                print(f"   è§£ç : {decoded}")
                return
        print(f"âŒ æœªæ‰¾åˆ° ID: {token_id}")
    else:
        # æŒ‰tokenå­—ç¬¦ä¸²æŸ¥æ‰¾
        if query in vocab:
            decoded = decode_token(query, reverse_map)
            print(f"\nğŸ“Œ Token: {query}")
            print(f"   ID: {vocab[query]}")
            print(f"   è§£ç : {decoded}")
        else:
            print(f"âŒ æœªæ‰¾åˆ° token: {query}")


def interactive_mode(vocab_path):
    """
    äº¤äº’å¼æŸ¥è¯¢æ¨¡å¼
    
    Args:
        vocab_path: vocab.jsonçš„è·¯å¾„
    """
    forward_map = bytes_to_unicode()
    reverse_map = get_reverse_mapping(forward_map)
    vocab = load_vocab(vocab_path)
    
    print("\nğŸ® è¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
    print("æç¤ºï¼šå¯ä»¥è¾“å…¥tokenå­—ç¬¦ä¸²æˆ–IDè¿›è¡ŒæŸ¥è¯¢")
    print("-" * 60)
    
    while True:
        try:
            query = input("\nğŸ” è¯·è¾“å…¥æŸ¥è¯¢ > ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not query:
                continue
            
            # æŒ‰IDæŸ¥è¯¢
            if query.isdigit():
                token_id = int(query)
                found = False
                for token_str, tid in vocab.items():
                    if tid == token_id:
                        decoded = decode_token(token_str, reverse_map)
                        print(f"  ğŸ“Œ ID: {token_id}")
                        print(f"     åŸå§‹: {repr(token_str)}")
                        print(f"     è§£ç : {decoded}")
                        found = True
                        break
                if not found:
                    print(f"  âŒ æœªæ‰¾åˆ° ID: {token_id}")
            else:
                # æŒ‰tokenå­—ç¬¦ä¸²æŸ¥è¯¢
                if query in vocab:
                    decoded = decode_token(query, reverse_map)
                    print(f"  ğŸ“Œ Token: {repr(query)}")
                    print(f"     ID: {vocab[query]}")
                    print(f"     è§£ç : {decoded}")
                else:
                    print(f"  âŒ æœªæ‰¾åˆ° token: {query}")
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"  âš ï¸  é”™è¯¯: {e}")


def main():
    parser = argparse.ArgumentParser(
        description='BPE ByteLevel Tokenizer Vocab è§£ç å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è§£ç æ•´ä¸ªvocabæ–‡ä»¶å¹¶æ˜¾ç¤ºå‰20ä¸ªæ ·æœ¬
  python3 decode_vocab.py ../model/vocab.json -n 20
  
  # è§£ç å¹¶ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
  python3 decode_vocab.py ../model/vocab.json -o ../model/vocab_decoded.json
  
  # æŸ¥è¯¢ç‰¹å®štoken (æŒ‰ID)
  python3 decode_vocab.py ../model/vocab.json -q 5892
  
  # æŸ¥è¯¢ç‰¹å®štoken (æŒ‰å­—ç¬¦ä¸²)
  python3 decode_vocab.py ../model/vocab.json -q "Ã§Ä¼Ä¦Ã¤Â¸Ä¯Ã¥Ä²Ä®"
  
  # äº¤äº’å¼æŸ¥è¯¢æ¨¡å¼
  python3 decode_vocab.py ../model/vocab.json -i
        """
    )
    
    parser.add_argument('vocab_path', help='vocab.jsonæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºè§£ç ç»“æœåˆ°æ–‡ä»¶')
    parser.add_argument('-n', '--samples', type=int, default=10, 
                        help='æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ï¼š10ï¼‰')
    parser.add_argument('-q', '--query', help='æŸ¥è¯¢ç‰¹å®štokenï¼ˆIDæˆ–å­—ç¬¦ä¸²ï¼‰')
    parser.add_argument('-i', '--interactive', action='store_true',
                        help='è¿›å…¥äº¤äº’å¼æŸ¥è¯¢æ¨¡å¼')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.vocab_path).exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {args.vocab_path}")
        return 1
    
    try:
        if args.interactive:
            # äº¤äº’æ¨¡å¼
            interactive_mode(args.vocab_path)
        elif args.query:
            # æŸ¥è¯¢æ¨¡å¼
            query_token(args.vocab_path, args.query)
        else:
            # è§£ç æ¨¡å¼
            decode_vocab(args.vocab_path, args.output, args.samples)
        
        return 0
    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
