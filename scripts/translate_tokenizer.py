#!/usr/bin/env python3
"""
Tokenizer ç¿»è¯‘å·¥å…·

å°† ByteLevel BPE tokenizer.json æ–‡ä»¶ä¸­çš„æ‰€æœ‰"ä¹±ç "tokenç¿»è¯‘ä¸ºåŸå§‹æ–‡æœ¬ã€‚
è¿™ä¼šä¿®æ”¹ tokenizer.json ä¸­çš„ vocabã€merges å’Œ added_tokens éƒ¨åˆ†ã€‚
"""

import json
import argparse
from pathlib import Path
import re


def bytes_to_unicode():
    """
    ç”Ÿæˆå­—èŠ‚åˆ°Unicodeå­—ç¬¦çš„æ­£å‘æ˜ å°„è¡¨
    è¿™ä¸ªæ˜ å°„æ¥è‡ª GPT-2 çš„ tokenizer å®ç°
    
    è¿”å›å­—å…¸ï¼š{byte_value: unicode_char}
    """
    # ä¿ç•™çš„å¯æ‰“å°å­—ç¬¦èŒƒå›´
    bs = (
        list(range(ord("!"), ord("~") + 1)) +      # ASCIIå¯æ‰“å°å­—ç¬¦ï¼ˆ33-126ï¼‰
        list(range(ord("Â¡"), ord("Â¬") + 1)) +      # æ‰©å±•å­—ç¬¦ï¼ˆ161-172ï¼‰
        list(range(ord("Â®"), ord("Ã¿") + 1))        # æ‰©å±•å­—ç¬¦ï¼ˆ174-255ï¼‰
    )
    
    cs = bs.copy()
    n = 0
    
    # å°†æœªè¢«ä¿ç•™çš„å­—èŠ‚æ˜ å°„åˆ°æ›´é«˜çš„Unicodeç ä½
    for b in range(2**8):  # éå†æ‰€æœ‰å­—èŠ‚ï¼ˆ0-255ï¼‰
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)  # æ˜ å°„åˆ°256+nçš„Unicodeç ä½
            n += 1
    
    # è½¬æ¢ä¸ºUnicodeå­—ç¬¦
    cs = [chr(code) for code in cs]
    
    return dict(zip(bs, cs))


def get_reverse_mapping(forward_map):
    """
    æ ¹æ®æ­£å‘æ˜ å°„ç”Ÿæˆåå‘æ˜ å°„
    
    è¿”å›å­—å…¸ï¼š{unicode_char: byte_value}
    """
    return {v: k for k, v in forward_map.items()}


def unicode_str_to_bytes(unicode_str, reverse_map):
    """
    å°†æ˜ å°„åçš„Unicodeå­—ç¬¦ä¸²è½¬æ¢å›åŸå§‹å­—èŠ‚åºåˆ—
    
    Args:
        unicode_str: æ˜ å°„åçš„Unicodeå­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼š"Ã¤Â½Å‚Ã¥Â¥Â½"ï¼‰
        reverse_map: åå‘æ˜ å°„è¡¨
    
    Returns:
        bytes: åŸå§‹å­—èŠ‚åºåˆ—
    """
    return bytes([reverse_map[c] for c in unicode_str])


def decode_token(token_str, reverse_map):
    """
    è§£ç å•ä¸ªtokenå­—ç¬¦ä¸²ä¸ºåŸå§‹æ–‡æœ¬
    
    Args:
        token_str: tokenå­—ç¬¦ä¸²ï¼ˆå¯èƒ½æ˜¯ä¹±ç ï¼‰
        reverse_map: åå‘æ˜ å°„è¡¨
    
    Returns:
        str: è§£ç åçš„åŸå§‹æ–‡æœ¬ï¼ˆå¦‚æœæ— æ³•è§£ç æˆ–è§£ç ä¸ºç©ºåˆ™è¿”å›åŸå­—ç¬¦ä¸²ï¼‰
    """
    try:
        # å°†Unicodeå­—ç¬¦ä¸²è½¬å›å­—èŠ‚
        byte_sequence = unicode_str_to_bytes(token_str, reverse_map)
        # å°è¯•è§£ç ä¸ºUTF-8æ–‡æœ¬
        decoded = byte_sequence.decode('utf-8', errors='ignore')
        # å¦‚æœè§£ç ç»“æœä¸ºç©ºï¼Œè¿”å›åŸå­—ç¬¦ä¸²
        if not decoded:
            return token_str
        return decoded
    except Exception:
        # å¦‚æœè§£ç å¤±è´¥ï¼Œè¿”å›åŸå­—ç¬¦ä¸²
        return token_str


def translate_vocab(vocab, reverse_map):
    """
    ç¿»è¯‘vocabå­—å…¸
    
    Args:
        vocab: {token: id} æ˜ å°„
        reverse_map: åå‘æ˜ å°„è¡¨
    
    Returns:
        dict: ç¿»è¯‘åçš„vocab
    """
    translated = {}
    for token, token_id in vocab.items():
        decoded = decode_token(token, reverse_map)
        translated[decoded] = token_id
    return translated


def translate_merges(merges, reverse_map):
    """
    ç¿»è¯‘mergesåˆ—è¡¨
    
    Args:
        merges: [[token1, token2], ...] æ ¼å¼çš„mergeè§„åˆ™åˆ—è¡¨
        reverse_map: åå‘æ˜ å°„è¡¨
    
    Returns:
        list: ç¿»è¯‘åçš„merges
    """
    translated = []
    for merge in merges:
        # merges æ ¼å¼å¯èƒ½æ˜¯ [token1, token2] æˆ– "token1 token2"
        if isinstance(merge, list) and len(merge) == 2:
            token1, token2 = merge
            decoded1 = decode_token(token1, reverse_map)
            decoded2 = decode_token(token2, reverse_map)
            translated.append([decoded1, decoded2])
        elif isinstance(merge, str):
            parts = merge.split(' ')
            if len(parts) == 2:
                token1, token2 = parts
                decoded1 = decode_token(token1, reverse_map)
                decoded2 = decode_token(token2, reverse_map)
                translated.append(f"{decoded1} {decoded2}")
            else:
                # ä¿æŒåŸæ ·
                translated.append(merge)
        else:
            # ä¿æŒåŸæ ·ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
            translated.append(merge)
    return translated


def translate_added_tokens(added_tokens, reverse_map):
    """
    ç¿»è¯‘added_tokensåˆ—è¡¨
    
    Args:
        added_tokens: [{"content": token, ...}, ...] æ ¼å¼çš„åˆ—è¡¨
        reverse_map: åå‘æ˜ å°„è¡¨
    
    Returns:
        list: ç¿»è¯‘åçš„added_tokens
    """
    translated = []
    for token_info in added_tokens:
        token_info_copy = token_info.copy()
        if 'content' in token_info_copy:
            content = token_info_copy['content']
            decoded = decode_token(content, reverse_map)
            token_info_copy['content'] = decoded
        translated.append(token_info_copy)
    return translated


def translate_tokenizer(input_path, output_path=None):
    """
    ç¿»è¯‘æ•´ä¸ªtokenizer.jsonæ–‡ä»¶
    
    Args:
        input_path: è¾“å…¥çš„tokenizer.jsonè·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰
    """
    print("ğŸ”„ åŠ è½½æ˜ å°„è¡¨...")
    forward_map = bytes_to_unicode()
    reverse_map = get_reverse_mapping(forward_map)
    
    print(f"ğŸ“– åŠ è½½tokenizer: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        tokenizer_data = json.load(f)
    
    print("âœ¨ ç¿»è¯‘tokenizer...")
    
    # ç¿»è¯‘ model.vocab
    if 'model' in tokenizer_data and 'vocab' in tokenizer_data['model']:
        print("  - ç¿»è¯‘ vocab...")
        original_vocab_size = len(tokenizer_data['model']['vocab'])
        tokenizer_data['model']['vocab'] = translate_vocab(
            tokenizer_data['model']['vocab'], 
            reverse_map
        )
        print(f"    âœ“ å·²ç¿»è¯‘ {original_vocab_size} ä¸ªtokens")
    
    # ç¿»è¯‘ model.merges
    if 'model' in tokenizer_data and 'merges' in tokenizer_data['model']:
        print("  - ç¿»è¯‘ merges...")
        original_merges_count = len(tokenizer_data['model']['merges'])
        tokenizer_data['model']['merges'] = translate_merges(
            tokenizer_data['model']['merges'], 
            reverse_map
        )
        print(f"    âœ“ å·²ç¿»è¯‘ {original_merges_count} ä¸ªmergeè§„åˆ™")
    
    # ç¿»è¯‘ added_tokens
    if 'added_tokens' in tokenizer_data:
        print("  - ç¿»è¯‘ added_tokens...")
        original_added_count = len(tokenizer_data['added_tokens'])
        tokenizer_data['added_tokens'] = translate_added_tokens(
            tokenizer_data['added_tokens'], 
            reverse_map
        )
        print(f"    âœ“ å·²ç¿»è¯‘ {original_added_count} ä¸ªç‰¹æ®Štokens")
    
    # ä¿å­˜ç»“æœ
    if output_path is None:
        output_path = input_path
    
    print(f"\nğŸ’¾ ä¿å­˜ç¿»è¯‘åçš„tokenizeråˆ°: {output_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_data, f, ensure_ascii=False, indent=2)
    
    print("âœ… ç¿»è¯‘å®Œæˆï¼")
    
    # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    if 'model' in tokenizer_data and 'vocab' in tokenizer_data['model']:
        vocab = tokenizer_data['model']['vocab']
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»è¯æ±‡é‡: {len(vocab)}")
        
        # ç»Ÿè®¡ä¸€äº›ç¤ºä¾‹
        print(f"\nğŸ“ ç¿»è¯‘ç¤ºä¾‹ (å‰10ä¸ª):")
        for i, (token, token_id) in enumerate(sorted(vocab.items(), key=lambda x: x[1])):
            if i >= 10:
                break
            print(f"   [{token_id}] {repr(token)}")


def main():
    parser = argparse.ArgumentParser(
        description='Tokenizer ç¿»è¯‘å·¥å…· - å°†ByteLevelç¼–ç çš„tokenç¿»è¯‘ä¸ºåŸå§‹æ–‡æœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ç¿»è¯‘tokenizer.jsonå¹¶ä¿å­˜åˆ°æ–°æ–‡ä»¶
  python3 translate_tokenizer.py ../model/tokenizer.json -o ../model/tokenizer_translated.json
  
  # ç¿»è¯‘å¹¶è¦†ç›–åŸæ–‡ä»¶ï¼ˆè°¨æ…ä½¿ç”¨ï¼å»ºè®®å…ˆå¤‡ä»½ï¼‰
  python3 translate_tokenizer.py ../model/tokenizer.json
  
æ³¨æ„:
  - è¿™ä¼šä¿®æ”¹tokenizerçš„å†…éƒ¨è¡¨ç¤ºï¼Œä½¿å…¶æ›´æ˜“è¯»
  - ç¿»è¯‘åçš„tokenizeråŠŸèƒ½ä¸Šåº”è¯¥ä¿æŒä¸€è‡´
  - å»ºè®®åœ¨ç¿»è¯‘å‰å¤‡ä»½åŸæ–‡ä»¶
        """
    )
    
    parser.add_argument('input_path', help='è¾“å…¥çš„tokenizer.jsonæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è¦†ç›–åŸæ–‡ä»¶ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    input_path = Path(args.input_path)
    if not input_path.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {args.input_path}")
        return 1
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œæç¤ºç”¨æˆ·
    if args.output is None:
        print("âš ï¸  è­¦å‘Š: æœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œå°†è¦†ç›–åŸæ–‡ä»¶ï¼")
        try:
            response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ").strip().lower()
            if response not in ['y', 'yes']:
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
                return 0
        except KeyboardInterrupt:
            print("\nâŒ æ“ä½œå·²å–æ¶ˆ")
            return 0
    
    try:
        translate_tokenizer(args.input_path, args.output)
        return 0
    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())

