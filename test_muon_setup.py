#!/usr/bin/env python3
"""
Quick test script to verify Muon implementation is working correctly.
Run this from the minimind directory: python test_muon_setup.py
"""

import sys
import torch
from pathlib import Path

# Colors for output
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    BOLD = '\033[1m'
    END = '\033[0m'

def print_test(name, passed, details=""):
    status = f"{Colors.GREEN}‚úÖ PASS{Colors.END}" if passed else f"{Colors.RED}‚ùå FAIL{Colors.END}"
    print(f"{status} {name}")
    if details:
        print(f"   {Colors.YELLOW}{details}{Colors.END}")

def main():
    print(f"\n{Colors.CYAN}{Colors.BOLD}{'='*70}")
    print("üî¨ MiniMind Muon Setup Verification")
    print(f"{'='*70}{Colors.END}\n")
    
    # Test 1: Import muon module
    try:
        from trainer.muon import Muon, DistMuon
        print_test("Import muon module", True, "Muon and DistMuon classes available")
    except Exception as e:
        print_test("Import muon module", False, str(e))
        return False
    
    # Test 2: Import model with QK norm
    try:
        from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
        print_test("Import model", True, "MiniMindForCausalLM imported successfully")
    except Exception as e:
        print_test("Import model", False, str(e))
        return False
    
    # Test 3: Create small model and check QK norm
    try:
        config = MiniMindConfig(hidden_size=256, num_hidden_layers=4, num_attention_heads=4, num_key_value_heads=2)
        model = MiniMindForCausalLM(config)
        
        # Check if QK norm exists
        has_q_norm = hasattr(model.model.layers[0].self_attn, 'q_norm')
        has_k_norm = hasattr(model.model.layers[0].self_attn, 'k_norm')
        
        if has_q_norm and has_k_norm:
            print_test("QK Normalization", True, "q_norm and k_norm layers detected in model")
        else:
            print_test("QK Normalization", False, "QK norm layers not found")
            return False
    except Exception as e:
        print_test("Create model", False, str(e))
        return False
    
    # Test 4: Separate parameters for Muon/AdamW
    try:
        muon_params = []
        adamw_params = []
        
        for name, param in model.named_parameters():
            if param.ndim == 2 and 'embed' not in name and 'norm' not in name and 'lm_head' not in name:
                muon_params.append(param)
            else:
                adamw_params.append(param)
        
        total_muon = sum(p.numel() for p in muon_params)
        total_adamw = sum(p.numel() for p in adamw_params)
        total_params = total_muon + total_adamw
        muon_pct = 100 * total_muon / total_params
        
        details = f"Muon: {total_muon:,} params ({muon_pct:.1f}%), AdamW: {total_adamw:,} params ({100-muon_pct:.1f}%)"
        print_test("Parameter separation", True, details)
        
        if muon_pct < 70 or muon_pct > 90:
            print(f"   {Colors.YELLOW}‚ö†Ô∏è  Warning: Muon should optimize 80-85% of params, got {muon_pct:.1f}%{Colors.END}")
    except Exception as e:
        print_test("Parameter separation", False, str(e))
        return False
    
    # Test 5: Create optimizers
    try:
        optimizer_muon = Muon(muon_params, lr=0.02, momentum=0.95, nesterov=True)
        optimizer_adamw = torch.optim.AdamW(adamw_params, lr=5e-4)
        print_test("Create optimizers", True, "Muon and AdamW optimizers created")
    except Exception as e:
        print_test("Create optimizers", False, str(e))
        return False
    
    # Test 6: Forward pass
    try:
        dummy_input = torch.randint(0, 100, (2, 32))
        output = model(dummy_input)
        print_test("Forward pass", True, f"Output shape: {output.logits.shape}")
    except Exception as e:
        print_test("Forward pass", False, str(e))
        return False
    
    # Test 7: Backward pass
    try:
        loss = output.logits.mean()
        loss.backward()
        
        # Check gradients exist
        has_grads = all(p.grad is not None for p in muon_params if p.requires_grad)
        if has_grads:
            print_test("Backward pass", True, "Gradients computed successfully")
        else:
            print_test("Backward pass", False, "Some parameters missing gradients")
            return False
    except Exception as e:
        print_test("Backward pass", False, str(e))
        return False
    
    # Test 8: Optimizer steps
    try:
        optimizer_muon.step()
        optimizer_adamw.step()
        optimizer_muon.zero_grad()
        optimizer_adamw.zero_grad()
        print_test("Optimizer steps", True, "Both optimizers stepped successfully")
    except Exception as e:
        print_test("Optimizer steps", False, str(e))
        return False
    
    # Test 9: Check training scripts exist
    trainer_path = Path("trainer")
    scripts = [
        "train_pretrain_muon.py",
        "train_full_sft_muon.py", 
        "train_dpo_muon.py"
    ]
    
    all_exist = True
    for script in scripts:
        if not (trainer_path / script).exists():
            all_exist = False
            print_test(f"Script: {script}", False, "File not found")
        else:
            print_test(f"Script: {script}", True, "File exists")
    
    # Test 10: Quick Newton-Schulz test
    try:
        from trainer.muon import zeropower_via_newtonschulz5
        test_matrix = torch.randn(64, 64)
        orthogonalized = zeropower_via_newtonschulz5(test_matrix, steps=5)
        
        # Check if result is approximately orthogonal
        identity_check = (orthogonalized @ orthogonalized.T).float()
        identity = torch.eye(64)
        error = (identity_check - identity).abs().max().item()
        
        if error < 0.1:
            print_test("Newton-Schulz iteration", True, f"Orthogonalization error: {error:.6f}")
        else:
            print_test("Newton-Schulz iteration", False, f"High error: {error:.6f}")
    except Exception as e:
        print_test("Newton-Schulz iteration", False, str(e))
        return False
    
    # Summary
    print(f"\n{Colors.CYAN}{Colors.BOLD}{'='*70}")
    print("‚ú® Summary")
    print(f"{'='*70}{Colors.END}\n")
    
    print(f"{Colors.GREEN}‚úÖ All tests passed!{Colors.END}")
    print(f"\n{Colors.BOLD}Your Muon setup is ready to use!{Colors.END}")
    print(f"\nNext steps:")
    print(f"  1. Read: {Colors.CYAN}MUON_IMPLEMENTATION_SUMMARY.md{Colors.END}")
    print(f"  2. Try: {Colors.CYAN}cd trainer && python train_pretrain_muon.py --help{Colors.END}")
    print(f"  3. Train: {Colors.CYAN}python train_pretrain_muon.py --use_wandb{Colors.END}")
    print(f"\n{Colors.YELLOW}üí° Expected speedup: 25-35% faster training!{Colors.END}\n")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}Test interrupted by user{Colors.END}")
        sys.exit(1)
    except Exception as e:
        print(f"\n{Colors.RED}Unexpected error: {e}{Colors.END}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

